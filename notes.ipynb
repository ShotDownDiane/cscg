{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation as a Dual Task of Code Summarization\n",
    "\n",
    "<img src='https://i.imgur.com/RqN1agC.png' width='600' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "- https://blog.floydhub.com/attention-mechanism/\n",
    "- https://github.com/Bolin0215/CSCGDual\n",
    "\n",
    "**Dataset:** https://github.com/wanyao1992/code_summarization_public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "$x \\; \\text{: code snippets}, \\; y \\; \\text{: comments}$\n",
    "\n",
    "$P(x,y) = \\color{#00a010}{P(x) \\cdot P(y|x)} = \\color{#a010a0}{P(y) \\cdot P(x|y)}$\n",
    "\n",
    "### Loss terms\n",
    "\n",
    "$l_{xy} = -\\frac{1}{m} \\sum_{t=1}^{m} P(y_t | y_{\\lt t}, x)$\n",
    "\n",
    "$l_{yx} = -\\frac{1}{n} \\sum_{t=1}^{n} P(x_t | x_{\\lt t}, y)$\n",
    "\n",
    "$l_{dual}=\\left[ \\left(\\color{#00a010}{\\log\\hat{P}(x) + \\log P(y \\vert x; \\theta_{xy})} \\right) - \\left(\\color{#a010a0}{\\log\\hat{P}(y) + \\log P(x \\vert y; \\theta_{yx})} \\right) \\right]^{2} \\text{ : regularization term}$\n",
    "\n",
    "$l_{att} = l_1 + l_2, \\text{ where } l_k = \\mathcal{D}_{JS} \\left( b_i, b_i' \\right ) = \\frac{1}{2n}\\sum_{i=1}^{n} \\mathcal{D}_{KL} \\left(b_i \\, || \\, \\frac{b_i + b_i'}{2} \\right) + \\mathcal{D}_{KL} \\left(b_i' \\, || \\, \\frac{b_i + b_i'}{2} \\right)$\n",
    "\n",
    "$b_i = softmax \\left( A_{xy}[i, :] \\right), \\; b_i' = softmax \\left( A_{yx}[i, :] \\right)$\n",
    "\n",
    "$A_{xy} \\in \\mathbb{R}^{n \\times m}, \\; A_{yx} \\in \\mathbb{R}^{m \\times n} \\text{ : attention weights}$\n",
    "\n",
    "### Updates\n",
    "\n",
    "$\\text{Minibatch of } k \\text{ pairs: } \\langle \\left(x_i, y_i\\right) \\rangle_{i=1}^{k}$\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "G_{xy} = \\nabla_{\\theta_{xy}} \\frac{1}{k} \\sum_{i=1}^{k} \\left( l_{xy} + \\lambda_{dual}^{(1)} \\cdot l_{dual} + \\lambda_{att}^{(1)} \\cdot l_{att} \\right)\\\\\n",
    "G_{yx} = \\nabla_{\\theta_{yx}} \\frac{1}{k} \\sum_{i=1}^{k} \\left( l_{yx} + \\lambda_{dual}^{(2)} \\cdot l_{dual} + \\lambda_{att}^{(2)} \\cdot l_{att} \\right)\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$\\text{Update } \\theta_{xy} \\text{ and } \\theta_{yx} \\text{ independently}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg' width='450' align='left'>\n",
    "<img src='https://i.stack.imgur.com/SjnTl.png' width='450' align='right'>\n",
    "<img src='https://camo.githubusercontent.com/48c57cd28983a819a5a7a0fe7119a269d8f86af6/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313230302f312a4752513931484e415342374d414a5054546c567666772e6a706567'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
