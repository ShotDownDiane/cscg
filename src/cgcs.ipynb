{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation as a Dual Task of Code Summarization\n",
    "\n",
    "```\n",
    "@article{wei2019code,\n",
    "  title={Code Generation as a Dual Task of Code Summarization},\n",
    "  author={Wei, Bolin and Li, Ge and Xia, Xin and Fu, Zhiyi and Jin, Zhi},\n",
    "  journal={arXiv preprint arXiv:1910.05923},\n",
    "  year={2019}\n",
    "}\n",
    "```\n",
    "\n",
    "<img src='https://i.imgur.com/RqN1agC.png' width='600' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "- https://blog.floydhub.com/attention-mechanism/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "$x \\; \\text{: code snippets}, \\; y \\; \\text{: comments}$\n",
    "\n",
    "$P(x,y) = \\color{#00a010}{P(x) \\cdot P(y|x)} = \\color{#a010a0}{P(y) \\cdot P(x|y)}$\n",
    "\n",
    "### Loss terms\n",
    "\n",
    "$l_{xy} = -\\frac{1}{m} \\sum_{t=1}^{m} P(y_t | y_{\\lt t}, x)$\n",
    "\n",
    "$l_{yx} = -\\frac{1}{n} \\sum_{t=1}^{n} P(x_t | x_{\\lt t}, y)$\n",
    "\n",
    "$l_{dual}=\\left[ \\left(\\color{#00a010}{\\log\\hat{P}(x) + \\log P(y \\vert x; \\theta_{xy})} \\right) - \\left(\\color{#a010a0}{\\log\\hat{P}(y) + \\log P(x \\vert y; \\theta_{yx})} \\right) \\right]^{2} \\text{ : regularization term}$\n",
    "\n",
    "$l_{att} = l_1 + l_2, \\text{ where } l_k = \\mathcal{D}_{JS} \\left( b_i, b_i' \\right ) = \\frac{1}{2n}\\sum_{i=1}^{n} \\mathcal{D}_{KL} \\left(b_i \\, || \\, \\frac{b_i + b_i'}{2} \\right) + \\mathcal{D}_{KL} \\left(b_i' \\, || \\, \\frac{b_i + b_i'}{2} \\right)$\n",
    "\n",
    "$b_i = softmax \\left( A_{xy}[i, :] \\right), \\; b_i' = softmax \\left( A_{yx}[i, :] \\right)$\n",
    "\n",
    "$A_{xy} \\in \\mathbb{R}^{n \\times m}, \\; A_{yx} \\in \\mathbb{R}^{m \\times n} \\text{ : attention weights}$\n",
    "\n",
    "### Updates\n",
    "\n",
    "$\\text{Minibatch of } k \\text{ pairs: } \\langle \\left(x_i, y_i\\right) \\rangle_{i=1}^{k}$\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "G_{xy} = \\nabla_{\\theta_{xy}} \\frac{1}{k} \\sum_{i=1}^{k} \\left( l_{xy} + \\lambda_{dual}^{(1)} \\cdot l_{dual} + \\lambda_{att}^{(1)} \\cdot l_{att} \\right)\\\\\n",
    "G_{yx} = \\nabla_{\\theta_{yx}} \\frac{1}{k} \\sum_{i=1}^{k} \\left( l_{yx} + \\lambda_{dual}^{(2)} \\cdot l_{dual} + \\lambda_{att}^{(2)} \\cdot l_{att} \\right)\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$\\text{Update } \\theta_{xy} \\text{ and } \\theta_{yx} \\text{ independently}$\n",
    "\n",
    "### Notes\n",
    "- The last encoder's hidden state is used to init the decoder's hidden state.\n",
    "\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from datasets import Django"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIR    = '/home/alex/workspace/msc-research/embeddings'\n",
    "DJANGO_DIR = '/home/alex/workspace/msc-research/raw-datasets/django/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP = Namespace()\n",
    "HP.batch_size = 5\n",
    "HP.epochs     = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP.dataset_config = Namespace()\n",
    "HP.dataset_config.__dict__ = {\n",
    "    'p_split': 0.8,\n",
    "    'anno_seq_maxlen': 40,\n",
    "    'code_seq_maxlen': 20,\n",
    "    'emb_file': os.path.join(EMB_DIR, 'glove.6B.50d.txt.pickle')\n",
    "}\n",
    "\n",
    "django = Django(root_dir=DJANGO_DIR, config=HP.dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, emb_matrix, hidden_size, input_maxlen, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.batch_size   = batch_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.input_maxlen = input_maxlen\n",
    "        \n",
    "        self.vocab_size, self.emb_dim = emb_matrix.shape\n",
    "        \n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, \n",
    "                                   output_dim=self.emb_dim, \n",
    "                                   embeddings_initializer=Constant(emb_matrix), \n",
    "                                   input_length=input_maxlen, \n",
    "                                   trainable=False)\n",
    "        \n",
    "        self.lstm = LSTM(self.hidden_size, \n",
    "                         return_sequences=True, \n",
    "                         return_state=True, \n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.bidir_lstm = Bidirectional(self.lstm, merge_mode='concat')\n",
    "\n",
    "    def call(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "        \n",
    "        out, h0, c0, h1, c1 = self.bidir_lstm(self.embedding(x), hidden)\n",
    "        \n",
    "        return out, (h0, c0), (h1, c1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        z = tf.zeros((self.batch_size, self.hidden_size))\n",
    "        return (z,z) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "enc = Encoder(emb_matrix=django.emb_matrix,\n",
    "             hidden_size=1024,\n",
    "             input_maxlen=HP.dataset_config.anno_seq_maxlen,\n",
    "             batch_size=bs)\n",
    "\n",
    "x_batch = np.array([django.x_train[i] for i in range(bs)])\n",
    "\n",
    "o, _, _ = enc(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong's Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, rnn_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        \n",
    "        self.W = tf.keras.layers.Dense(rnn_size)\n",
    "\n",
    "    def call(self, decoder_output, encoder_output):\n",
    "        # score: h_t x W x h_s\n",
    "        # encoder_output (h_s) shape: (batch_size, max_len, rnn_size)\n",
    "        # decoder_output (h_t) shape: (batch_size, 1, rnn_size)\n",
    "        # score will have shape: (batch_size, 1, max_len)\n",
    "        \n",
    "        score = tf.matmul(decoder_output, self.W(encoder_output), transpose_b=True)\n",
    "        alignment = tf.nn.softmax(score, axis=2)\n",
    "        context = tf.matmul(alignment, encoder_output)\n",
    "\n",
    "        return context, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = LuongAttention(20)\n",
    "bs = 1\n",
    "e = np.random.rand(bs, 10, 20).astype('f')\n",
    "d = np.random.rand(bs, 1, 20).astype('f')\n",
    "c, a = att(d, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = LuongAttention(self.hidden_size)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([x, tf.expand_dims(context_vector, 1)], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.lstm(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(emb_matrix=django.emb_matrix,\n",
    "                  hidden_size=1024,\n",
    "                  input_maxlen=HP.dataset_config.anno_seq_maxlen,\n",
    "                  batch_size=HP.batch_size)\n",
    "\n",
    "decoder = Decoder(vocab_size=100,\n",
    "                  embedding_dim=30,\n",
    "                  hidden_size=1024,\n",
    "                  batch_size=HP.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(HP.epochs):\n",
    "    t_start = timer()\n",
    "\n",
    "    enc_hidden = encoder.init_hidden()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy() :.5f}')\n",
    "        \n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch :.5f}')\n",
    "    print(f'Time taken for 1 epoch {timer() - t_start :.4f} sec\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
